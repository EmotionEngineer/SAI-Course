{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e66a2bb3",
   "metadata": {
    "papermill": {
     "duration": 0.003961,
     "end_time": "2024-12-04T19:03:53.068183",
     "exception": false,
     "start_time": "2024-12-04T19:03:53.064222",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸ“Š Processing Large CSV Files in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc155b19",
   "metadata": {
    "papermill": {
     "duration": 0.002538,
     "end_time": "2024-12-04T19:03:53.073751",
     "exception": false,
     "start_time": "2024-12-04T19:03:53.071213",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "This notebook demonstrates how to handle large CSV/TSV files efficiently using Python. \n",
    "We will implement two key functionalities:\n",
    "1. **Batch Processing with Column Removal**: Load and process data in chunks to save memory, and optionally remove unnecessary columns.\n",
    "2. **Sampling Data**: Extract a specific percentage of rows, either sequentially or randomly, for quick analysis.\n",
    "\n",
    "Both methods ensure memory efficiency while handling datasets that may not fit into memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d77fb42f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T19:03:53.081524Z",
     "iopub.status.busy": "2024-12-04T19:03:53.081120Z",
     "iopub.status.idle": "2024-12-04T19:03:54.016731Z",
     "shell.execute_reply": "2024-12-04T19:03:54.015603Z"
    },
    "papermill": {
     "duration": 0.94273,
     "end_time": "2024-12-04T19:03:54.019417",
     "exception": false,
     "start_time": "2024-12-04T19:03:53.076687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3537513",
   "metadata": {
    "papermill": {
     "duration": 0.003019,
     "end_time": "2024-12-04T19:03:54.025490",
     "exception": false,
     "start_time": "2024-12-04T19:03:54.022471",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Batch Processing Large CSV Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6015bb0a",
   "metadata": {
    "papermill": {
     "duration": 0.002543,
     "end_time": "2024-12-04T19:03:54.030789",
     "exception": false,
     "start_time": "2024-12-04T19:03:54.028246",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "This function processes a large CSV/TSV file in smaller batches, removes unnecessary columns, \n",
    "and combines the processed data into a final output file. Temporary files are created during processing \n",
    "and are deleted after the final result is saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b32333d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T19:03:54.038673Z",
     "iopub.status.busy": "2024-12-04T19:03:54.038124Z",
     "iopub.status.idle": "2024-12-04T19:03:54.048398Z",
     "shell.execute_reply": "2024-12-04T19:03:54.047255Z"
    },
    "papermill": {
     "duration": 0.016898,
     "end_time": "2024-12-04T19:03:54.050493",
     "exception": false,
     "start_time": "2024-12-04T19:03:54.033595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_large_csv_in_batches(\n",
    "    file_path, \n",
    "    columns_to_drop, \n",
    "    batch_size=10000, \n",
    "    output_folder='temp_batches', \n",
    "    final_output='final_data.pkl',\n",
    "    sep='\t'\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a large CSV file in batches.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the input CSV/TSV file.\n",
    "    - columns_to_drop (list): List of column names to drop from each batch.\n",
    "    - batch_size (int): Number of rows to process in each batch.\n",
    "    - output_folder (str): Temporary folder to store processed batches.\n",
    "    - final_output (str): Path to save the final combined output.\n",
    "    - sep (str): Delimiter used in the CSV/TSV file.\n",
    "    \"\"\"\n",
    "    # Create a temporary folder for batch storage\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Initialize batch counter\n",
    "    batch_count = 0\n",
    "    \n",
    "    # Load the file in chunks\n",
    "    for chunk in tqdm(pd.read_csv(file_path, sep=sep, chunksize=batch_size), desc=\"Processing Batches\"):\n",
    "        # Remove specified columns\n",
    "        chunk = chunk.drop(columns=columns_to_drop, errors='ignore')\n",
    "        \n",
    "        # Save the processed batch to a pickle file\n",
    "        batch_file = os.path.join(output_folder, f'batch_{batch_count}.pkl')\n",
    "        chunk.to_pickle(batch_file)\n",
    "        \n",
    "        # Clear memory\n",
    "        del chunk\n",
    "        \n",
    "        # Increment batch counter\n",
    "        batch_count += 1\n",
    "        print(f\"Batch {batch_count} processed and saved.\")\n",
    "\n",
    "    # Combine all batch files into a single DataFrame\n",
    "    all_batches = glob.glob(os.path.join(output_folder, 'batch_*.pkl'))\n",
    "    final_df = pd.concat([pd.read_pickle(batch) for batch in all_batches], ignore_index=True)\n",
    "    \n",
    "    # Save the combined result to a single file\n",
    "    final_df.to_pickle(final_output)\n",
    "    print(f\"Final data saved to {final_output}\")\n",
    "    \n",
    "    # Clear memory\n",
    "    del final_df\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    for batch in all_batches:\n",
    "        os.remove(batch)\n",
    "    os.rmdir(output_folder)\n",
    "    print(\"Temporary files deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98c97d20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T19:03:54.058176Z",
     "iopub.status.busy": "2024-12-04T19:03:54.057809Z",
     "iopub.status.idle": "2024-12-04T19:04:04.135425Z",
     "shell.execute_reply": "2024-12-04T19:04:04.134197Z"
    },
    "papermill": {
     "duration": 10.084,
     "end_time": "2024-12-04T19:04:04.137708",
     "exception": false,
     "start_time": "2024-12-04T19:03:54.053708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 1it [00:01,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 2it [00:03,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2 processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 3it [00:04,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3 processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 4it [00:05,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4 processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 5it [00:07,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5 processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 7it [00:08,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6 processed and saved.\n",
      "Batch 7 processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final data saved to final_data.pkl\n",
      "Temporary files deleted.\n"
     ]
    }
   ],
   "source": [
    "# Example usage for batch processing\n",
    "process_large_csv_in_batches(\n",
    "    file_path='/kaggle/input/vk-salary-train/train.tsv',  # Update path as needed\n",
    "    columns_to_drop=['raw_description', 'raw_branded_description'],\n",
    "    batch_size=10000,\n",
    "    output_folder='temp_batches',\n",
    "    final_output='final_data.pkl',\n",
    "    sep='\\t'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171480b2",
   "metadata": {
    "papermill": {
     "duration": 0.003546,
     "end_time": "2024-12-04T19:04:04.145537",
     "exception": false,
     "start_time": "2024-12-04T19:04:04.141991",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sampling Data from a Large CSV File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4885f170",
   "metadata": {
    "papermill": {
     "duration": 0.003633,
     "end_time": "2024-12-04T19:04:04.152913",
     "exception": false,
     "start_time": "2024-12-04T19:04:04.149280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "This function samples a subset of rows from a large CSV/TSV file. The sampling can be:\n",
    "- **Sequential**: Extract the first N% of rows.\n",
    "- **Random**: Select a random N% of rows without loading the entire file into memory.\n",
    "\n",
    "The sampled data is saved into a separate CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40610be3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T19:04:04.161988Z",
     "iopub.status.busy": "2024-12-04T19:04:04.161581Z",
     "iopub.status.idle": "2024-12-04T19:04:04.169790Z",
     "shell.execute_reply": "2024-12-04T19:04:04.168733Z"
    },
    "papermill": {
     "duration": 0.015245,
     "end_time": "2024-12-04T19:04:04.171876",
     "exception": false,
     "start_time": "2024-12-04T19:04:04.156631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_csv(\n",
    "    file_path, \n",
    "    percentage=10, \n",
    "    random_sample=False, \n",
    "    output_file='sampled_data.csv',\n",
    "    sep=','\n",
    "):\n",
    "    \"\"\"\n",
    "    Samples a specific percentage of data from a large CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the input CSV/TSV file.\n",
    "    - percentage (int): Percentage of rows to sample (1-100).\n",
    "    - random_sample (bool): If True, randomly sample rows; otherwise, sample sequentially.\n",
    "    - output_file (str): Path to save the sampled data.\n",
    "    - sep (str): Delimiter used in the CSV/TSV file.\n",
    "    \"\"\"\n",
    "    if percentage <= 0 or percentage > 100:\n",
    "        raise ValueError(\"Percentage must be between 1 and 100.\")\n",
    "    \n",
    "    # Calculate the total number of rows\n",
    "    total_rows = sum(1 for _ in open(file_path)) - 1  # Subtract 1 for the header\n",
    "    sample_size = int(total_rows * (percentage / 100))\n",
    "    \n",
    "    if random_sample:\n",
    "        # Randomly sample rows without reading the entire file\n",
    "        skip_rows = sorted(np.random.choice(range(1, total_rows + 1), total_rows - sample_size, replace=False))\n",
    "        sampled_df = pd.read_csv(file_path, sep=sep, skiprows=skip_rows)\n",
    "    else:\n",
    "        # Sequentially sample the first N% rows\n",
    "        sampled_df = pd.read_csv(file_path, sep=sep, nrows=sample_size)\n",
    "    \n",
    "    # Save the sampled data to a new CSV file\n",
    "    sampled_df.to_csv(output_file, index=False)\n",
    "    print(f\"Sampled data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bde8edd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T19:04:04.181309Z",
     "iopub.status.busy": "2024-12-04T19:04:04.180934Z",
     "iopub.status.idle": "2024-12-04T19:04:08.208266Z",
     "shell.execute_reply": "2024-12-04T19:04:08.206879Z"
    },
    "papermill": {
     "duration": 4.035355,
     "end_time": "2024-12-04T19:04:08.211109",
     "exception": false,
     "start_time": "2024-12-04T19:04:04.175754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled data saved to sampled_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Example usage for sampling\n",
    "sample_csv(\n",
    "    file_path='/kaggle/input/vk-salary-train/train.tsv',  # Update path as needed\n",
    "    percentage=10,\n",
    "    random_sample=True,\n",
    "    output_file='sampled_data.csv',\n",
    "    sep='\\t'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6045067,
     "sourceId": 9851613,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6045069,
     "sourceId": 9851616,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18.490621,
   "end_time": "2024-12-04T19:04:08.737156",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-04T19:03:50.246535",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
